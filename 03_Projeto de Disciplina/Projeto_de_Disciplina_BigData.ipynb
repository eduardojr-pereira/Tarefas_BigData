{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projeto_de_Disciplina_BigData.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZHi2MioHi75J",
        "h-vuEe1f2Hw-",
        "pWk_bqQrAn7_",
        "BDJyXv8xa3qI",
        "2YGy3YF2a_jZ",
        "h4h0xwvNanwJ",
        "vuKOp4zfkF_R",
        "pNjNr65BBfuG",
        "7ApR0z4AkWVq",
        "X0zGCQ6H3bM2",
        "3Xx1A28o3ev7",
        "YzBjZKny3hgC",
        "_cRLUoic_LWh",
        "1b4jxnXT_K1A",
        "MFUcvPEyAbn7",
        "I7fwTDmDMzoN",
        "xlftitdkM1vm"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Projeto de Disciplina** - BigData e Processamento Distribuído\n",
        "**Alunos:**\n",
        "* Eduardo Junior Pereira\n",
        "* Gabriel Ramalho Grotto\n",
        "___\n",
        "# 1. Visão geral e objetivos\n",
        "**Objetivo geral:** \n",
        "> O objetivo desse projeto é realizar um ciclo completo de ciência de dados utilizando PySpark.\n",
        "\n",
        "**Objetivos específicos:**\n",
        "> *Parte I: Exploração de Dados:* \n",
        "* realizar no mínimo **2 análises** (estatística, análise \n",
        "com gráficos, ...);\n",
        "* realizar no mínimo **3 transformações** (filtragem, remoção de características, remoção/troca de calores nulos, normalizações, ...).\n",
        "\n",
        "> *Parte II: Criação de um Modelo e Análise de Resultados:* \n",
        "* rodar um algoritmo da biblioteca MLlib do Spark; \n",
        "* motivar a escolha do algoritmo; \n",
        "* dividir os dados utilizando alguma metodologia de validação (*cross-validation*, 60-40, 80-20, ...); \n",
        "* validar a performance do modelo e analisar os resultados.\n",
        "___\n",
        "\n",
        "# 2. Dataset\n",
        " \n",
        "Para a realização dos objetivos propostos, foi escolhido o seguinte conjunto de dados:\n",
        "\n",
        ">**Título do Dataset:** Credit Approval \n",
        "* *Disponível em:* [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/credit+approval \"UCI Machine Learning Repository - Fonte do Dataset\") \n",
        "* *Número total de registros:* 690 \n",
        "* *Número total de features:* 15 + variável alvo \n",
        "* [Descrição das features](http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.names \"Clique para fazer o download do .txt com a descrição das features\")  \n",
        "___\n",
        "\n",
        ">**Informações relevantes**\n",
        "* Os valores no conjunto de dados foram convertidos em símbolos para proteger a confidencialidade dos dados.\n",
        "* O conjunto de dados refere-se a pedidos de crédito e, nesse sentido, ao longo desse trabalho procurou-se contribuir para a discussão acerca das decisões de uma empresa em aprovar ou negar esses pedidos.\n",
        "* A análise consiste em uma tarefa de `classificação binária` e, em suma, procurou-se avaliar a performance do algoritmo de classificação `Logistic Regression`.\n",
        "* O algoritmo está localizados no módulo `pyspark.ml.classification` e para maiores informações acessar a [documentação.](https://spark.apache.org/docs/latest/ml-classification-regression.html#classification \"Clique para acessar a documentação.\")"
      ],
      "metadata": {
        "id": "yGPfabfRXuu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "# 3. Configuração do projeto\n",
        "> 3.1. Instalar PySpark no Google Colab\n",
        ">\n",
        "> 3.2. Importar bibliotecas e inicializar uma  sessão local - *SparkSession*\n",
        ">\n",
        "> 3.3. Carregar os dados\n",
        "___"
      ],
      "metadata": {
        "id": "rOf0ljaR0JBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Instalar PySpark no Google Colab\n",
        "> **Fonte:** [Google Colab](https://colab.research.google.com/github/carlosfab/sigmoidal_ai/blob/master/Big_Data_Como_instalar_o_PySpark_no_Google_Colab.ipynb \"Como instalar o PySpark no Google Colab \")"
      ],
      "metadata": {
        "id": "ZHi2MioHi75J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Atualizacao necessária para a execucao no Google Colab\n",
        "!apt-get update\n",
        "# Instalar as dependências necessárias para a execução no Google Colab\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "print('-'*52+'\\nAtualização e instalação de dependências concluídas!\\n'+'-'*52)"
      ],
      "metadata": {
        "id": "NBXqm74RiXzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Importar bibliotecas "
      ],
      "metadata": {
        "id": "GvUIceWq2C1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tornar o pyspark \"importável\"\n",
        "import findspark\n",
        "findspark.init('spark-3.2.0-bin-hadoop3.2')\n",
        "\n",
        "# Importar pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.functions import col, sum, count, when, isnan\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, RobustScaler, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics \n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# Bibliotecas necessárias para a manipulação dos dados\n",
        "import os\n",
        "import urllib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Bibliotecas necessárias para a visualização dos dados\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import matplotlib.lines as mlines\n",
        "import seaborn as sns\n",
        "\n",
        "# Ignorar warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "emh-GQvv2CY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Inicializar uma sessão local (*SparkSession*) e carregar os dados"
      ],
      "metadata": {
        "id": "h-vuEe1f2Hw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar uma sessão local\n",
        "spark = SparkSession.builder.appName('crx-data-pyspark').getOrCreate()\n",
        "\n",
        "# Download do http para arquivo local\n",
        "!wget --quiet 'http://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data'\n",
        "\n",
        "# Carregar os dados\n",
        "df_spark = spark.read.csv('/content/crx.data', header=False)\n",
        "\n",
        "# Visualizar algumas informações sobre os tipos de dados de cada coluna\n",
        "print(\"-\"*50+\"\\nInformações sobre os tipos de dados de cada coluna\\n\"+\"-\"*50+\n",
        "      f'\\nTotal de linhas: {df_spark.count()}\\n'+\n",
        "      f'Total de colunas: {len(df_spark.columns)}\\n'+'-'*50)\n",
        "df_spark.printSchema()\n",
        "\n",
        "# Visualizar as cinco primeiras linhas do dataframe\n",
        "df_spark.show(5)"
      ],
      "metadata": {
        "id": "Dip05tnqyAJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "# 4. Parte I - Exploração dos dados\n",
        "> 4.1. Renomear as colunas do dataframe\n",
        ">\n",
        "> 4.2. Verificar a existência registros nulos e ausentes\n",
        ">\n",
        "> 4.3. Análise e tratamento das variáveis categóricas\n",
        ">\n",
        "> 4.4. Distribuição dos dados\n",
        ">\n",
        "> 4.5. Verificar a correlação entre as variáveis\n",
        ">\n",
        "> 4.6. Extração de variáveis\n",
        ">\n",
        "> 4.7. Particionamento do conjunto de dados\n",
        ">\n",
        "> 4.8. Escalonamento dos dados - `RobustScaler`\n",
        ">\n",
        "> 4.9. `VectorAssembler`\n",
        "___"
      ],
      "metadata": {
        "id": "pWk_bqQrAn7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Renomear as colunas do dataframe\n",
        "\n"
      ],
      "metadata": {
        "id": "BDJyXv8xa3qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rename_columns(df, columns):\n",
        "    \"\"\"\n",
        "    Função para renomear as colunas do dataframe\n",
        "    ====Parâmetros====\n",
        "    df -- dataframe no qual as colunas serão renomeadas\n",
        "    columns -- dicionário com os nomes das colunas, onde:\n",
        "        * As CHAVES do dicionário correspondem aos NOMES ANTIGOS das colunas; \n",
        "        * Os VALORES de cada chave correspondem aos NOVOS NOMES que serão \n",
        "        atribuídos às colunas.\n",
        "    \"\"\"\n",
        "    if isinstance(columns, dict):\n",
        "        for old_name, new_name in columns.items():\n",
        "            df = df.withColumnRenamed(old_name, new_name)\n",
        "        return df\n",
        "    else:\n",
        "        raise ValueError(\"'columns' deve ser um dicionário do tipo\\n {'old_name_1':'new_name_1', 'old_name_2':'new_name_2'}\")\n",
        "\n",
        "new_columns = {'_c0':'Gender','_c1':'Age','_c2':'Debt','_c3':'Married',\n",
        "               '_c4':'BankCustomer','_c5':'EducationLevel','_c6':'Ethnicity',\n",
        "               '_c7':'YearsEmployed','_c8':'PriorDefault','_c9':'Employed',\n",
        "               '_c10':'CreditScore','_c11':'DriversLicense','_c12':'Citizen',\n",
        "               '_c13':'ZipCode','_c14':'Income','_c15':'ApprovalStatus'}\n",
        "\n",
        "df_spark = rename_columns(df_spark, new_columns)\n",
        "df_spark.show(5)"
      ],
      "metadata": {
        "id": "PUL55xLbAmF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Verificar a existência registros nulos e ausentes"
      ],
      "metadata": {
        "id": "2YGy3YF2a_jZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar a existência de valores nulos \n",
        "print('-'*35+'\\nTotal de valores nulos por coluna:\\n'+'-'*35)\n",
        "df_spark.select([count(when(col(c).isNull(), c)).alias(c) for c in df_spark.columns]).show(1, vertical=True)"
      ],
      "metadata": {
        "id": "-5RLJzQZqdiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar a existência de valores ausentes \n",
        "print('-'*37+'\\nTotal de valores ausentes por coluna:\\n'+'-'*37)\n",
        "df_spark.select([count(when(isnan(c), c)).alias(c) for c in df_spark.columns]).show(1, vertical=True)"
      ],
      "metadata": {
        "id": "5sHzeRk2engO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. Análise e tratamento das variáveis categóricas"
      ],
      "metadata": {
        "id": "h4h0xwvNanwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar variáveis categóricas\n",
        "for cols in new_columns.values():\n",
        "    print('-'*(len(cols)+44)+f'\\nVerificando se \"{cols}\" é uma variável categórica:\\n'+'-'*(len(cols)+44))\n",
        "    df_spark.groupBy(cols).count().show()"
      ],
      "metadata": {
        "id": "S4GJkk1MBE0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "___\n",
        "**Notas:**\n",
        "> Durante a checagem das variáveis categóricas, observou-se a existência de registros iguais a **\"?\"**\n",
        "para algumas dessas variáveis. São elas:\n",
        "* Gender\n",
        "* Married\n",
        "* BankCustomer\n",
        "* EducationLevel\n",
        "* Ethinicity\n",
        ">\n",
        "> Uma vez que os registros iguais a **\"?\"** representam uma pequena fração do total do conjunto de dados, optou-se por excluir as linhas nas quais esses valores aparecem.\n",
        "___\n",
        "___"
      ],
      "metadata": {
        "id": "m-KJeE1wGxJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Excluir linhas com os registros iguais a \"?\" \n",
        "df_spark = df_spark.where(\"Gender!='?'\")\n",
        "df_spark = df_spark.where(\"Married!='?'\")\n",
        "df_spark = df_spark.where(\"BankCustomer!='?'\")\n",
        "df_spark = df_spark.where(\"EducationLevel!='?'\")\n",
        "df_spark = df_spark.where(\"Ethnicity!='?'\")\n",
        "\n",
        "# Verificar novamente a existência de registros iguais a ?\n",
        "for cols in ['Gender','Married','BankCustomer','EducationLevel','Ethnicity']:\n",
        "    print('-'*(len(cols)+64)+f'\\nVerificando a existência de registros iguais a \"?\" na coluna \"{cols}\":\\n'+'-'*(len(cols)+64))\n",
        "    df_spark.select(cols).distinct().show()"
      ],
      "metadata": {
        "id": "XR18ShD6I_9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Indexação das variáveis categóricas\n",
        "\n",
        "# Criar dicionário do tipo {'input_cols':'output_cols'}\n",
        "dict_cols = {'Gender':'Gender_idx','Married':'Married_idx',\n",
        "             'BankCustomer':'BankCustomer_idx',\n",
        "             'EducationLevel':'EducationLevel_idx',\n",
        "             'Ethnicity':'Ethnicity_idx','PriorDefault':'PriorDefault_idx',\n",
        "             'Employed':'Employed_idx', 'CreditScore':'CreditScore_idx',\n",
        "             'DriversLicense':'DriversLicense_idx','Citizen':'Citizen_idx',\n",
        "             'ApprovalStatus':'Y_bin'}\n",
        "\n",
        "indexer = StringIndexer(inputCols=list(dict_cols.keys()), \n",
        "                        outputCols=list(dict_cols.values()))\n",
        "\n",
        "df_spark_idx = indexer.fit(df_spark).transform(df_spark)\n",
        "\n",
        "# Verificar indexação das variáveis categóricas\n",
        "for keys, values  in dict_cols.items():\n",
        "    print('-'*(len(keys)+40)+f'\\nVerificando a indexação da varriável \"{keys}\":\\n'+'-'*(len(keys)+40))\n",
        "    df_spark_idx.select(keys, values).distinct().orderBy(values, ascending=True).show()"
      ],
      "metadata": {
        "id": "8pRidUIlL9mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "___\n",
        "**Notas:**\n",
        "> Algumas features possuem mais de dois valores e para evitar que o algoritmo atribua importância maior aos pesos maiores, decidiu-se utilizar o `OneHotEncoder`:\n",
        ">\n",
        "> Features categóricas que serão codificadas utilizando o OneHotEncoder:\n",
        "* Married\n",
        "* BankCustomer\n",
        "* EducationLevel\n",
        "* Ethinicity\n",
        "* CreditScore\n",
        "* Citizen\n",
        "___\n",
        "___"
      ],
      "metadata": {
        "id": "4_hXWLBqSf9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Codificação dummy das variáveis categóricas utilizando OneHotEncoder\n",
        "# Criar dicionário do tipo {'input_cols':'output_cols'}\n",
        "dict_cols_oh = {'Married_idx':'Married_oh','BankCustomer_idx':'BankCustomer_oh',\n",
        "                'EducationLevel_idx':'EducationLevel_oh','Ethnicity_idx':'Ethnicity_oh',\n",
        "                'CreditScore_idx':'CreditScore_oh','Citizen_idx':'Citizen_oh'}\n",
        "\n",
        "onehot = OneHotEncoder(inputCols=list(dict_cols_oh.keys()),\n",
        "                       outputCols=list(dict_cols_oh.values()))\n",
        "\n",
        "df_spark_oh = onehot.fit(df_spark_idx).transform(df_spark_idx)\n",
        "\n",
        "# Verificar representação dummy das variáveis categóricas\n",
        "for keys, values  in dict_cols_oh.items():\n",
        "    print('-'*(len(keys)+50)+f'\\nVerificando a representação dummy da varriável \"{keys}\":\\n'+'-'*(len(keys)+50))\n",
        "    df_spark_oh.select(keys, values).distinct().orderBy(keys, ascending=True).show()"
      ],
      "metadata": {
        "id": "NM7tdy3lRYTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pré-seleção de features - excluir apenas variáveis sem a representação dummy\n",
        "cols_to_drop_oh = ['Gender','Married','BankCustomer','EducationLevel','Ethnicity',\n",
        "                   'PriorDefault','Employed','CreditScore','DriversLicense','Citizen',\n",
        "                   'ApprovalStatus','Married_idx','BankCustomer_idx','EducationLevel_idx',\n",
        "                   'Ethnicity_idx','CreditScore_idx','Citizen_idx']\n",
        "df_spark_oh = df_spark_oh.drop(*cols_to_drop_oh)\n",
        "\n",
        "# Modificar o Dtype das features\n",
        "# Criar dicionário do tipo {'feature':'dtype'}\n",
        "dict_dtypes = {'Age':'float', 'Debt':'float','YearsEmployed':'float',\n",
        "               'ZipCode':'integer','Income':'float'}\n",
        "for keys, values in dict_dtypes.items():\n",
        "    df_spark_oh = df_spark_oh.withColumn(keys, col(keys).cast(values))\n",
        "\n",
        "df_spark_oh.printSchema()"
      ],
      "metadata": {
        "id": "nwVpvs-8XLkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4. Distribuição dos dados"
      ],
      "metadata": {
        "id": "vuKOp4zfkF_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar estatísticas descritivas das features do dataframe\n",
        "print('-'*42+'\\nEstatísticas descritivas da variável alvo:\\n'+'-'*42)\n",
        "df_spark_oh.select('Y_bin').describe().show()\n",
        "\n",
        "print('-'*50+'\\nEstatísticas descritivas das variáveis preditoras:\\n'+'-'*50)\n",
        "df_spark_oh.drop('Y_bin').describe().show()"
      ],
      "metadata": {
        "id": "J5jOXko3WXY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequência de ocorrência da variável alvo\n",
        "aproved = df_spark_oh.where('Y_bin == 1').count()\n",
        "not_aproved = df_spark_oh.where('Y_bin == 0').count()\n",
        "\n",
        "# Criar canvas para plotar os gráficos\n",
        "fig1 = plt.figure(constrained_layout=False, figsize=(10,5))\n",
        "fig1.suptitle('Figura 01 - Distribuição de Frequência da Variável Alvo.\\n'+'-'*76)\n",
        "# Criar gridspec\n",
        "gs = gridspec.GridSpec(nrows=1, ncols=2, hspace=0.2, wspace=0)\n",
        "# Parâmetros gerais para plotar os gráficos\n",
        "sizes = [aproved,not_aproved] \n",
        "labels=['Pedidos Aprovados (dummy=1)','Pedidos Negados (dummy=0)']\n",
        "colors=['paleturquoise','lightcoral']\n",
        "\n",
        "# Plotar barras com o total de cada ocorrência da variável alvo - ax1[0,0]\n",
        "ax1 = fig1.add_subplot(gs[0,0])\n",
        "ax1.get_xaxis().set_visible(False)\n",
        "ax1.set_xlabel(f'Total de ocorrências', fontsize=10)\n",
        "ax1.set_yticklabels([1,0])\n",
        "ax1.spines['top'].set_visible(False)\n",
        "ax1.spines['right'].set_visible(False)\n",
        "ax1.spines['bottom'].set_visible(False)\n",
        "ax1.invert_yaxis()\n",
        "plt.barh(labels, sizes, color=colors, height=0.5)\n",
        "for idx in ax1.patches:\n",
        "    plt.text(idx.get_width(), idx.get_y() + 0.2,\n",
        "             str(int(idx.get_width())),\n",
        "             fontweight='bold')\n",
        "\n",
        "# Plotar gráfico donut com % de ocorrência da variável alvo - ax2[0,1]\n",
        "ax2 = fig1.add_subplot(gs[0,1])\n",
        "ax2.pie(x=sizes, colors=colors,autopct='%.2f%%', pctdistance=0.83,shadow=True, \n",
        "        startangle=270, explode=(0.03,0.03), textprops={'fontweight':'bold'})\n",
        "ax2.legend(labels=labels, loc='center', bbox_to_anchor=(0,0),ncol=2)\n",
        "# Criar círculo branco para plotar o donut\n",
        "white_circle = plt.Circle((0,0),0.7,color='white')\n",
        "p = plt.gcf()\n",
        "_=p.gca().add_artist(white_circle)\n",
        "\n",
        "plt.savefig(\"Figura01_DistFrequenciaVariavelAlvo.png\", transparent=True)"
      ],
      "metadata": {
        "id": "meimGWO3AUxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "___\n",
        "**Notas:**\n",
        "> De acordo com o gráfico acima, podemos observar que houve um total de **671** registros, dos quais **300** *foram aprovados* e **371** *negados*, o que por sua vez correspondem respectivamente aos percentuais de 55,29% e 44,71% do total de pedidos registrados.\n",
        ">\n",
        "> O próximo passo consiste em verificar a distribuição de frequência das variáveis: **Age, Debt, YearsEmployed, ZipCode, Income.**\n",
        "___\n",
        "___"
      ],
      "metadata": {
        "id": "gVB03juPDeAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar objeto pandasDataFrame \n",
        "df_pd = df_spark_oh.toPandas()\n",
        "\n",
        "# Extrair nome das features\n",
        "cols_names = list(df_pd.columns[0:5])\n",
        "\n",
        "# Extrair df para pedidos_aprovados e pedidos_negados\n",
        "df_aprovados = df_pd[df_pd['Y_bin'] == 1]\n",
        "df_negados = df_pd[df_pd['Y_bin'] == 0]\n",
        "\n",
        "# Criar canvas para plotar os gráficos\n",
        "fig2 = plt.figure(constrained_layout=False, figsize=(10,15))\n",
        "fig2.suptitle('Figura 02 - Distribuição de Frequência das Variáveis Preditoras.\\n'+'-'*87)\n",
        "fig2.text(0.05,0.5, \"Count\", ha=\"center\", va=\"center\", rotation=90, size='x-large')\n",
        "# Criar gridspec\n",
        "gs = gridspec.GridSpec(nrows=5, ncols=3, hspace=0.25, wspace=0)\n",
        "\n",
        "# Plotar gráficos\n",
        "for a in range(5):\n",
        "    ax = fig2.add_subplot(gs[a,:2])\n",
        "    # Plotar histogramas\n",
        "    for idx, ax in enumerate(fig2.axes):\n",
        "        ax.figure\n",
        "        binwidth = (max(df_aprovados[cols_names[idx]])-min(df_negados[cols_names[idx]]))/50\n",
        "        ax.hist([df_aprovados[cols_names[idx]], df_negados[cols_names[idx]]],\n",
        "                bins = np.arange(min(df_pd[cols_names[idx]]),\n",
        "                                 max(df_pd[cols_names[idx]])+binwidth, binwidth),\n",
        "                alpha=0.7, stacked=True, label=labels, color=colors)\n",
        "        ax.set_xlabel(cols_names[idx])\n",
        "        ax.grid(axis='both',color='black',alpha=0.5,linestyle='-.',linewidth=0.5)\n",
        "\n",
        "# Plotar BoxPlot para variável 'Age'\n",
        "ax6 = fig2.add_subplot(gs[0,-1])\n",
        "ax6 = sns.boxplot(y=df_pd['Age'],notch=True, width=0.5, color='firebrick')\n",
        "ax6.set_ylabel('')\n",
        "ax6.get_yaxis().set_visible(False)\n",
        "ax6.set_frame_on(False)\n",
        "\n",
        "# Plotar BoxPlot para variável 'Debt'\n",
        "ax7 = fig2.add_subplot(gs[1,-1])\n",
        "ax7 = sns.boxplot(y=df_pd['Debt'],notch=True, width=0.5, color='cadetblue')\n",
        "ax7.set_ylabel('')\n",
        "ax7.get_yaxis().set_visible(False)\n",
        "ax7.set_frame_on(False)\n",
        "\n",
        "# Plotar BoxPlot para variável 'YearsEmployed'\n",
        "ax8 = fig2.add_subplot(gs[2,-1])\n",
        "ax8 = sns.boxplot(y=df_pd['YearsEmployed'],notch=True, width=0.5, color='lime')\n",
        "ax8.set_ylabel('')\n",
        "ax8.get_yaxis().set_visible(False)\n",
        "ax8.set_frame_on(False)\n",
        "\n",
        "# Plotar BoxPlot para variável 'ZipCode'\n",
        "ax9 = fig2.add_subplot(gs[3,-1])\n",
        "ax9 = sns.boxplot(y=df_pd['ZipCode'],notch=True, width=0.5, color='lightcoral')\n",
        "ax9.set_ylabel('')\n",
        "ax9.get_yaxis().set_visible(False)\n",
        "ax9.set_frame_on(False)\n",
        "\n",
        "# Plotar BoxPlot para variável 'Income'\n",
        "ax10 = fig2.add_subplot(gs[4,-1])\n",
        "ax10 = sns.boxplot(y=df_pd['Income'],notch=True, width=0.5, color='wheat')\n",
        "ax10.set_ylabel('')\n",
        "ax10.get_yaxis().set_visible(False)\n",
        "ax10.set_frame_on(False)\n",
        "\n",
        "fig2.legend(labels=['Pedidos Aprovados (dummy=1)','Pedidos Negados (dummy=0)'], \n",
        "            ncol=2, loc='upper center', bbox_to_anchor=(0.47,0.85))\n",
        "plt.savefig(\"Figura02_DistFrequenciaVariaveisPreditoras.png\", transparent=True)"
      ],
      "metadata": {
        "id": "3T8aeajbfWID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "___\n",
        "**Notas:**\n",
        "> De acordo com a Figura 02 é possível observar uma grande quantidade de *Outliers* para as variáveis analisadas.\n",
        "___\n",
        "___"
      ],
      "metadata": {
        "id": "shfAgsATcmO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5. Verificar a correlação entre as variáveis"
      ],
      "metadata": {
        "id": "pNjNr65BBfuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar multicolinearidade entre as variáveis\n",
        "# Plotar PairPlot para as variáveis preditoras e variável alvo\n",
        "pairplot = sns.pairplot(df_pd,corner=True, diag_kind='kde',kind='reg',\n",
        "                        plot_kws={'line_kws':{'color':'red'},'marker':'+'})\n",
        "pairplot.fig.suptitle('Figura 03 - Pairplot para as Variáveis Preditoras e Variável Alvo.\\n'+'-'*87, fontsize=20)\n",
        "\n",
        "plt.show()\n",
        "pairplot.figure.savefig(\"Figura03_Multicolinearidade-PairPlot.png\", transparent=True)"
      ],
      "metadata": {
        "id": "AOvNnAk9LQNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar canvas para plotar os gráficos\n",
        "fig4 = plt.figure(constrained_layout=False, figsize=(15,8))\n",
        "fig4.suptitle('Figura 04 - Correlação de Pearson entre as Variáveis Preditoras e Variável Alvo.\\n'+'-'*110)\n",
        "\n",
        "# Criar gridspec\n",
        "gs = gridspec.GridSpec(nrows=1, ncols=2, hspace=0.5)\n",
        "\n",
        "# Plotar heatmap - Criar matriz de correlação de Pearson\n",
        "matriz = df_pd.drop('Y_bin', axis=1)\n",
        "matriz = df_pd.corr()\n",
        "# Criar uma paleta de divergência (s=saturação, l=luminosidade, n=numero cores)\n",
        "cmap = sns.diverging_palette(250, 15, s=75, l=40, n=9,center=\"light\", as_cmap=True)\n",
        "# Criar uma máscara\n",
        "mascara = np.triu(np.ones_like(matriz, dtype=bool))\n",
        "ax1 = fig4.add_subplot(gs[0,0])\n",
        "ax1 = sns.heatmap(matriz, mask=mascara, center=0, fmt='.2f', square=True, \n",
        "                  cmap=cmap,cbar=False, linewidths=1, annot=True, \n",
        "                  annot_kws={'fontsize':10,'fontweight':'bold'})\n",
        "\n",
        "# Plotar gráfico de barras com as correlações\n",
        "ax2 = fig4.add_subplot(gs[0,1])\n",
        "ax2 = df_pd.corrwith(df_pd['Y_bin']).plot(kind='barh', color='wheat')\n",
        "ax2.invert_yaxis()\n",
        "ax2.yaxis.tick_right()\n",
        "ax2.spines['top'].set_visible(False)\n",
        "ax2.spines['left'].set_visible(False)\n",
        "ax2.grid(axis='both',color='black',alpha=0.5,linestyle='-.',linewidth=0.5)\n",
        "plt.xlim([-1,1])\n",
        "# Acrescentar reta com \"correlação média\"\n",
        "mean = np.mean(matriz['Y_bin'])\n",
        "ax2.axvline(mean, ls='--', color='sienna')\n",
        "ax2.axvline(x=0, color=\"grey\")\n",
        "\n",
        "red_line = mlines.Line2D([],[],color='sienna',linestyle='--', \n",
        "                         label='Média da correlação entre as variáveis = %.5f' %mean,)\n",
        "ax2.legend(handles=[red_line], loc='lower center', bbox_to_anchor=(0.5,-0.15))\n",
        "\n",
        "# Criar anotações nas barras\n",
        "for i in ax2.patches:\n",
        "    plt.text(i.get_width() - 0.1,\n",
        "             i.get_y() + 0.25,\n",
        "             str(round((i.get_width()), 5)),\n",
        "             fontsize=10,\n",
        "             fontweight='bold',\n",
        "             color='black',\n",
        "             va='center')\n",
        "# Determinar tamanho da fonte no eixo y\n",
        "for tick in ax2.yaxis.get_ticklabels():\n",
        "    tick.set_fontsize('10')\n",
        "    \n",
        "plt.savefig(\"Figura04_CorrelacaoPearson.png\", transparent=True)"
      ],
      "metadata": {
        "id": "MHREesSlenPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "___\n",
        "**Notas:**\n",
        ">* De acordo com as Figuras 03 e 04, podemos sugerir a presença de alta correlação entre as variáveis `Age` e `YearsEmployed`, podendo indicar padrões de multicolinearidade. \n",
        ">* A variável `YearsEmployed` também apresentou significativa correlação com a variável alvo `Y_bin` e com a variável `Debt`.\n",
        ">* As variáveis `Income` e `Employed_idx` também apresentaram correlação significativa entre elas e com a variável alvo `Y_bin`.\n",
        "___\n",
        "___\n"
      ],
      "metadata": {
        "id": "zMZQ8WWPJNJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6. Extração de variáveis"
      ],
      "metadata": {
        "id": "7ApR0z4AkWVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "___\n",
        "**Notas:**\n",
        ">* Com a intenção de evitar erros no processo de classificação, decidiu-se por excluir as variáveis:\n",
        "    * `Age` \n",
        "    * `ZipCode`  \n",
        "    * `Gender_idx` \n",
        "    * `DriversLicense_idx`\n",
        "    * `Married_oh`,\n",
        "    * `BankCustomer_oh`\n",
        "    * `Ethnicity_oh`\n",
        "    * `Citizen_oh`. \n",
        ">* Dentre as motivações para excluí-las podemos citar:\n",
        "    1. Existência de correlação significativa com outra variável preditora ou com a variável alvo;\n",
        "    2. Evidência de um grande número de *outliers*;\n",
        "    3. Evitar vieses de classifação por gênero, etnia ou similares.\n",
        "___\n",
        "___\n"
      ],
      "metadata": {
        "id": "NfMv39p71zQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleção de features\n",
        "cols_to_drop = ['Age','ZipCode','Gender_idx','DriversLicense_idx','Married_oh',\n",
        "                'BankCustomer_oh','Ethnicity_oh','Citizen_oh']\n",
        "\n",
        "df_ready_spark = df_spark_oh.drop(*cols_to_drop)\n",
        "df_ready_spark.show(5)"
      ],
      "metadata": {
        "id": "FYj0GcSKOU1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.7. Particionamento do conjunto de dados"
      ],
      "metadata": {
        "id": "X0zGCQ6H3bM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Particionar os dados\n",
        "df_train, df_test = df_ready_spark.randomSplit([0.8, 0.2], seed=123)\n",
        "\n",
        "print('-'*32+'\\nConjunto de dados de TREINAMENTO\\n'+'-'*32+\n",
        "      f'\\nTotal de linhas: {df_train.count()}\\n'+\n",
        "      f'Total de colunas: {len(df_train.columns)}\\n'+'-'*32+\n",
        "      '\\n\\n'+'-'*26+'\\nConjunto de dados de TESTE\\n'+'-'*26+\n",
        "      f'\\nTotal de linhas: {df_test.count()}\\n'+\n",
        "      f'Total de colunas: {len(df_test.columns)}\\n'+'-'*26)"
      ],
      "metadata": {
        "id": "_EEpSIUkg89g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.8. Escalonamento dos dados"
      ],
      "metadata": {
        "id": "3Xx1A28o3ev7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "___\n",
        "**Notas**\n",
        ">* Devido a presença de *outliers* no conjunto de dados decidiu-se por usar o `RobustScaler`. \n",
        ">* O RobustScaler remove a mediana e dimensiona os dados de acordo com o **intervalo de quantil**. \n",
        ">* O intervalo de quantil é, por padrão, um intervalo de quantil entre:\n",
        ">  * **1º quartil** = 25º quantil;  \n",
        ">  * e o **3º quartil** = 75º quantil  \n",
        "\n",
        "___\n",
        "___"
      ],
      "metadata": {
        "id": "SilWXTBGo178"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contruir o pipeline para o RobustScaler\n",
        "cols_to_scale = ['Debt','YearsEmployed','Income']\n",
        "assemblers = [VectorAssembler(inputCols=[col],outputCol=col+'_vec') for col in cols_to_scale]\n",
        "scalers = [RobustScaler(inputCol=col+'_vec',outputCol=col+'_scaled') for col in cols_to_scale] \n",
        "pipeline = Pipeline(stages = assemblers + scalers)\n",
        "\n",
        "# Rodando o pipeline \n",
        "scalerModel = pipeline.fit(df_train)\n",
        "df_train_scaled = scalerModel.transform(df_train)\n",
        "df_test_scaled = scalerModel.transform(df_test)\n",
        "\n",
        "# Verificar df_train_scaled\n",
        "print('-'*30+'\\nConjunto de dados de ORIGINAIS\\n'+'-'*30)\n",
        "df_train.show(5)\n",
        "# Verificar df_train\n",
        "print('-'*32+'\\nConjunto de dados de ESCALONADOS\\n'+'-'*32)\n",
        "df_train_scaled.show(5)"
      ],
      "metadata": {
        "id": "cscwKayzi45S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.9. VectorAssembler"
      ],
      "metadata": {
        "id": "YzBjZKny3hgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "___\n",
        "**Notas:**\n",
        ">* Spark requer que toda a informação que será passada para um algoritmo de ML seja convertida em um único vetor.\n",
        ">* Para fazer isso, usaremos `VectorAssembler`.\n",
        "___\n",
        "___"
      ],
      "metadata": {
        "id": "IzXYjNtv3xgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_use = ['PriorDefault_idx','Employed_idx','EducationLevel_oh',\n",
        "               'CreditScore_oh','Debt_scaled','YearsEmployed_scaled','Income_scaled']\n",
        "\n",
        "vec = VectorAssembler(inputCols=cols_to_use, outputCol='features')\n",
        "\n",
        "df_train_vec = vec.transform(df_train_scaled).select('features','Y_bin')\n",
        "df_test_vec = vec.transform(df_test_scaled).select('features','Y_bin')\n",
        "\n",
        "# Verificar schema() do df_train_vec\n",
        "print('-'*32+'\\nConjunto de dados de TREINAMENTO\\n'+\n",
        "      f'Total de registros: {df_train_vec.count()}\\n'+'-'*32)\n",
        "df_train_vec.printSchema()\n",
        "\n",
        "# Verificar schema() do df_test_vec\n",
        "print('-'*26+'\\nConjunto de dados de TESTE\\n'+\n",
        "      f'Total de registros: {df_test_vec.count()}\\n'+'-'*26)\n",
        "df_test_vec.printSchema()"
      ],
      "metadata": {
        "id": "lZxnMvUD3jYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "# 5. Parte II - Criação de um Modelo e Análise de Resultados\n",
        "> 5.1. LogisticRegression\n",
        "___"
      ],
      "metadata": {
        "id": "dMWwxHDNAtB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1. LogisticRegression"
      ],
      "metadata": {
        "id": "cuuu4bSLorgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "___\n",
        "**Notas:**\n",
        ">* A regressão logística é um método popular para prever uma resposta categórica. É um caso especial de modelos *Lineares Generalizados* que prevê a probabilidade dos resultados. \n",
        "* Em `spark.ml`, a regressão logística pode ser usada para prever um resultado binário usando regressão logística binomial ou pode ser usada para prever um resultado multiclasse usando regressão logística multinomial. \n",
        "* [Documentação](https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression \"Clique para acessar a documentação - LogisticRegression\")\n",
        "___\n",
        "___\n"
      ],
      "metadata": {
        "id": "CM7BVihWqV79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criar modelo"
      ],
      "metadata": {
        "id": "_cRLUoic_LWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciar classificador\n",
        "lr = LogisticRegression(featuresCol='features', labelCol='Y_bin')\n",
        "# Realizar treinamento com o conjunto de dados de treinamento\n",
        "lrModel = lr.fit(df_train_vec)\n",
        "# Realizar predições com o conjunto de dados de teste\n",
        "predictions_lr = lrModel.transform(df_test_vec)\n",
        "# Criar objeto para armazenar as predições\n",
        "preds_and_labels_lr = predictions_lr.select(['prediction','Y_bin'])\n",
        "preds_and_labels_lr.show(10)"
      ],
      "metadata": {
        "id": "JOjA2Wtv-lCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Avaliar modelo"
      ],
      "metadata": {
        "id": "1b4jxnXT_K1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar canvas para plotar os gráficos\n",
        "fig5 = plt.figure(constrained_layout=True, figsize=(12,5))\n",
        "fig5.suptitle('Figura 05 - Análise dos resultados sem validação cruzada.\\n'+\n",
        "              'Classificador = LogisticRegression()\\n'+'-'*50)\n",
        "# Criar gridspec\n",
        "gs = gridspec.GridSpec(nrows=1, ncols=3, hspace=0.2, wspace=0.3)\n",
        "\n",
        "# Plotar ROC Curve\n",
        "ax1 = fig5.add_subplot(gs[0,0])\n",
        "ax1.plot(lrModel.summary.roc.select('FPR').collect(),\n",
        "         lrModel.summary.roc.select('TPR').collect())\n",
        "ax1.set_title('ROC Curve')\n",
        "ax1.set_xlabel('False Positive Rate')\n",
        "ax1.set_ylabel('True Positive Rate')\n",
        "\n",
        "# Plotar Precision-Recall Curve\n",
        "ax2 = fig5.add_subplot(gs[0,1])\n",
        "ax2.plot(lrModel.summary.pr.select('recall').collect(),\n",
        "         lrModel.summary.pr.select('precision').collect())\n",
        "ax2.set_title('Precision-Recall Curve')\n",
        "ax2.set_xlabel('Recall')\n",
        "ax2.set_ylabel('Precision')\n",
        "\n",
        "# Plotar principais métricas de avaliação\n",
        "ax3 = fig5.add_subplot(gs[0,-1])\n",
        "beta = np.sort(lrModel.coefficients)\n",
        "ax3.plot(beta)\n",
        "ax3.set_ylabel('Beta Coefficients')\n",
        "\n",
        "plt.subplots_adjust(top=0.75)\n",
        "plt.savefig(\"Figura05_ROC_PR_Crurves-LogisticRegression.png\", transparent=True)"
      ],
      "metadata": {
        "id": "vsejiBDIzbeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Métricas para avaliar desempenho em treinamento\n",
        "result_train = {'Recall':lrModel.summary.weightedRecall,\n",
        "                'Precision':lrModel.summary.weightedPrecision,\n",
        "                'F1-Score':lrModel.summary.weightedFMeasure(),\n",
        "                'AreaUnderROC':lrModel.summary.areaUnderROC}\n",
        "df_results = pd.DataFrame(index=list(k for k in result_train.keys()))\n",
        "df_results['Desempenho em Treino']= list(v for v in result_train.values())\n",
        "\n",
        "# Instanciar avaliadores\n",
        "metrics_binary = BinaryClassificationMetrics(preds_and_labels_lr.rdd.map(tuple))\n",
        "metrics_multi = MulticlassMetrics(preds_and_labels_lr.rdd.map(tuple))\n",
        "# Métricas para avaliar desempenho em teste\n",
        "result_test = {'Recall':metrics_multi.recall(1.0),\n",
        "               'Precision':metrics_multi.precision(1.0),\n",
        "               'F1-Score':metrics_multi.fMeasure(1.0),\n",
        "               'AreaUnderROC':metrics_binary.areaUnderROC}\n",
        "\n",
        "df_results['Desempenho em Teste']= list(v for v in result_test.values())\n",
        "df_results.head()"
      ],
      "metadata": {
        "id": "LYeEx3dmhxDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tentativa de otimização utilizando validação cruzada"
      ],
      "metadata": {
        "id": "MFUcvPEyAbn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">* Spark oferece também ferramentas de otimização e seleção de hiperparâmetros para ML, dentro do módulo [`pyspark.ml.tuning`](https://spark.apache.org/docs/latest/ml-tuning.html \"Clique para acessar a documentação - pyspark.ml.tuning\"). Muitos algoritmos possuem vários hiperparâmetros que muitas das vezes só conseguem ser definidos empiricamente, rodando diversas configurações ou empregando técnicas de _busca em grade_. \n",
        "* Para uma busca em grade - `GridSearch`, Spark oferece o objeto `ParamGridBuilder`. Esse recurso deve ser empregado em conjunto com um mecanismo de _Validação Cruzada_, executado com um `CrossValidator`. \n",
        "* A Validação Cruzada é uma maneira muito eficiente de fazer otimização de parâmetros, pois é executada em diversas repartições dos dados de treinamento e oferece diversas visões de como um modelo se comporta. \n"
      ],
      "metadata": {
        "id": "fPr9Bj5yA04a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar grid \n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.01,0.1,0.5,1]) \\\n",
        "    .addGrid(lr.elasticNetParam, [0, 0.5, 1]) \\\n",
        "    .addGrid(lr.maxIter, [100,500,1000]).build()\n",
        "\n",
        "# Rodar validação cruzada\n",
        "crossval = CrossValidator(estimator=lr,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=bin_eval,\n",
        "                          numFolds=5)\n",
        "# Realizar treinamento com o conjunto de dados de treinamento\n",
        "cvModel_lr = crossval.fit(df_train_vec)\n",
        "# Realizar predições com o conjunto de dados de teste\n",
        "predictions_cv_lr = cvModel_lr.transform(df_test_vec)\n",
        "# Criar objeto para armazenar as predições\n",
        "preds_and_labels_cv_lr = predictions_cv_lr.select(['prediction','Y_bin'])"
      ],
      "metadata": {
        "id": "3R17No4DA0QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciar avaliadores\n",
        "metrics_binary_cv = BinaryClassificationMetrics(preds_and_labels_cv_lr.rdd.map(tuple))\n",
        "metrics_multi_cv = MulticlassMetrics(preds_and_labels_cv_lr.rdd.map(tuple))\n",
        "# Métricas para avaliar desempenho em teste com CrossValidator\n",
        "result_test = {'Recall':metrics_multi_cv.recall(1.0),\n",
        "               'Precision':metrics_multi_cv.precision(1.0),\n",
        "               'F1-Score':metrics_multi_cv.fMeasure(1.0),\n",
        "               'AreaUnderROC':metrics_binary_cv.areaUnderROC}\n",
        "\n",
        "df_results['Desempenho em Teste - CrossValidator']= list(v for v in result_test.values())\n",
        "df_results.head()"
      ],
      "metadata": {
        "id": "T2ka6mfqMFX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig6 = plt.figure(constrained_layout=False, figsize=(12,5))\n",
        "fig6.suptitle('Figura 06 - Matrizes de confusão_LogisticRegression().\\n'+'-'*75)\n",
        "# Criar gridspec\n",
        "gs = gridspec.GridSpec(nrows=1, ncols=2)\n",
        "\n",
        "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "\n",
        "# Plotar matriz de confusão sem CrossValidator\n",
        "# Calcular matriz de confusão\n",
        "cf_lr = metrics_multi.confusionMatrix().toArray()\n",
        "# Plotar\n",
        "ax1 = fig6.add_subplot(gs[0,0]) \n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in cf_lr.flatten()]\n",
        "group_percentages = [\"{0:0.2%}\".format(value) for value in cf_lr.flatten()/np.sum(cf_lr)]\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "ax1 = sns.heatmap(cf_lr, annot=labels, fmt='', cmap='Blues')\n",
        "ax1.set_title('Sem CrossValidator')\n",
        "ax1.set_xlabel('Predicted label')\n",
        "ax1.set_ylabel('True label')\n",
        "\n",
        "# Plotar matriz de confusão com CrossValidator\n",
        "# Calcular matriz de confusão\n",
        "cf_cv_lr = metrics_multi_cv.confusionMatrix().toArray()\n",
        "# Plotar\n",
        "ax2 = fig6.add_subplot(gs[0,1])\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in cf_cv_lr.flatten()]\n",
        "group_percentages = [\"{0:0.2%}\".format(value) for value in cf_cv_lr.flatten()/np.sum(cf_cv_lr)]\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "ax1 = sns.heatmap(cf_cv_lr, annot=labels, fmt='', cmap='Blues')\n",
        "ax1.set_title('Com CrossValidator')\n",
        "ax1.set_xlabel('Predicted label')\n",
        "ax1.set_ylabel('True label')\n",
        "\n",
        "plt.subplots_adjust(top=0.85)\n",
        "plt.savefig(\"Figura06_MatrizesConfusao-LogisticRegression.png\", transparent=True)"
      ],
      "metadata": {
        "id": "DyO5wlPnkBk9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}